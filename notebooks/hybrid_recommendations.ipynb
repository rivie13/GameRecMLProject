{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f770a071",
   "metadata": {},
   "source": [
    "# Hybrid Recommendation System (Advanced)\n",
    "\n",
    "## Goal\n",
    "Combine **ML predictions** with **content-based scoring** to create a hybrid recommendation system with **advanced filtering and user preference support**.\n",
    "\n",
    "## What's New vs. Basic Content-Based?\n",
    "This notebook includes ALL advanced features from model_development.ipynb:\n",
    "\n",
    "**ðŸ”’ Hard Filters** (auto-exclude):\n",
    "- NSFW content filter\n",
    "- Early Access filter  \n",
    "- Meta genre/tag filtering (Indie, Casual, Utilities, etc.)\n",
    "- Minimum review thresholds\n",
    "- User-specified hard exclusions\n",
    "\n",
    "**ðŸŽ¯ Smart Scoring**:\n",
    "- Tag matching weighted by YOUR playtime (45%)\n",
    "- Genre matching (20%)\n",
    "- Median playtime signal (20%)\n",
    "- Review quality tiers (15%)\n",
    "- **Disliked tag/genre penalties** (learns from low-playtime games)\n",
    "- **User preference boosts** (optional manual adjustments)\n",
    "\n",
    "## Approach\n",
    "1. Load trained ML model from feature_engineering.ipynb\n",
    "2. Build content-based scoring system with all advanced features\n",
    "3. Generate recommendations with 3 approaches:\n",
    "   - **Pure ML**: Use only ML predictions\n",
    "   - **Pure Content-Based**: Use only similarity scores (with all filters/penalties)\n",
    "   - **Hybrid**: Weighted combination of both\n",
    "4. Compare all 3 approaches side-by-side\n",
    "5. Analyze which approach gives best recommendations\n",
    "\n",
    "## Scoring Formula\n",
    "```\n",
    "Content Score = \n",
    "  + 45% Tag Matching (weighted by playtime, excludes NSFW/meta)\n",
    "  + 20% Genre Matching (excludes meta genres)\n",
    "  + 20% Median Playtime (engagement signal)\n",
    "  + 15% Review Quality (tiered by score, volume bonus)\n",
    "  - 10 points per disliked tag (soft penalty)\n",
    "  - 15 points per disliked genre (soft penalty)\n",
    "  \n",
    "Hybrid Score = 0.40*ML + 0.40*Content + 0.20*Review\n",
    "```\n",
    "\n",
    "*(Weights can be tuned based on results)*\n",
    "\n",
    "**Dislike Learning**: Games you own but never play (<5 hours) reveal disliked tags/genres, which get penalized in recommendations.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f214eec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import joblib\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "print(\"Libraries loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944f9076",
   "metadata": {},
   "source": [
    "## 2. Load Data\n",
    "\n",
    "Load all necessary datasets:\n",
    "- User's owned games (with engagement scores)\n",
    "- Steam catalog (candidates for recommendations)\n",
    "- Trained ML model and scaler\n",
    "- ML recommendations (from feature_engineering.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700f5ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "owned_games = pd.read_csv('../data/owned_games_enriched.csv')\n",
    "steam_catalog = pd.read_csv('../data/steam_catalog_detailed.csv')\n",
    "ml_recommendations_v2 = pd.read_csv('../data/ml_recommendations_v2_diverse.csv')\n",
    "\n",
    "print(f\"Owned games: {len(owned_games)}\")\n",
    "print(f\"Steam catalog: {len(steam_catalog)}\")\n",
    "print(f\"ML recommendations loaded: {len(ml_recommendations_v2)}\")\n",
    "\n",
    "# Identify owned game IDs\n",
    "owned_appids = set(owned_games['appid'].values)\n",
    "catalog_unowned = steam_catalog[~steam_catalog['appid'].isin(owned_appids)].copy()\n",
    "\n",
    "print(f\"\\nCatalog games you don't own: {len(catalog_unowned)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a652116",
   "metadata": {},
   "source": [
    "## 3. Load/Prepare ML Predictions\n",
    "\n",
    "We already have ML predictions from feature_engineering.ipynb.  \n",
    "Let's normalize them to 0-100 scale for consistency with content-based scoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a387bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ML recommendations already have 'predicted_engagement_v2' column (0-100 scale)\n",
    "# Let's verify the range\n",
    "print(\"ML Prediction Statistics:\")\n",
    "print(ml_recommendations_v2['predicted_engagement_v2'].describe())\n",
    "\n",
    "# Normalize to 0-100 if needed (should already be in this range)\n",
    "ml_scores = ml_recommendations_v2[['appid', 'name', 'predicted_engagement_v2']].copy()\n",
    "ml_scores.rename(columns={'predicted_engagement_v2': 'ml_score'}, inplace=True)\n",
    "\n",
    "print(f\"\\nML scores prepared for {len(ml_scores)} games\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa24d232",
   "metadata": {},
   "source": [
    "## 4. Build Content-Based Scoring System\n",
    "\n",
    "Implement content-based filtering using:\n",
    "1. **Tag similarity** to user's loved games (45%)\n",
    "2. **Genre overlap** (20%)\n",
    "3. **Median playtime similarity** (20%)\n",
    "4. **Review quality** (15%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf254b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_tags(tag_string):\n",
    "    \"\"\"Parse tag string into dictionary\"\"\"\n",
    "    if pd.isna(tag_string):\n",
    "        return {}\n",
    "    try:\n",
    "        return ast.literal_eval(str(tag_string))\n",
    "    except:\n",
    "        return {}\n",
    "\n",
    "def parse_genre(genre_string):\n",
    "    \"\"\"Parse genre string into list\"\"\"\n",
    "    if pd.isna(genre_string):\n",
    "        return []\n",
    "    return [g.strip() for g in str(genre_string).split(',')]\n",
    "\n",
    "# Parse tags and genres for both datasets\n",
    "owned_games['tags_dict'] = owned_games['tags'].apply(parse_tags)\n",
    "owned_games['genre_list'] = owned_games['genre'].apply(parse_genre)\n",
    "\n",
    "catalog_unowned['tags_dict'] = catalog_unowned['tags'].apply(parse_tags)\n",
    "catalog_unowned['genre_list'] = catalog_unowned['genre'].apply(parse_genre)\n",
    "\n",
    "print(\"Tags and genres parsed for content-based scoring\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68997514",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build user profile from loved games (50+ hours) AND identify disliked games\n",
    "loved_games = owned_games[owned_games['playtime_forever'] > 3000].copy()  # 50+ hours\n",
    "\n",
    "# Also identify disliked games (games owned but barely/never played)\n",
    "disliked_games = owned_games[owned_games['playtime_forever'] < 300].copy()  # <5 hours\n",
    "\n",
    "print(f\"Your loved games (50+ hours): {len(loved_games)}\")\n",
    "print(f\"Your disliked games (<5 hours): {len(disliked_games)}\")\n",
    "\n",
    "print(f\"\\n\\nTop 10 loved games:\")\n",
    "for _, game in loved_games.nlargest(10, 'playtime_forever')[['name', 'playtime_forever']].iterrows():\n",
    "    print(f\"  - {game['name']}: {game['playtime_forever']/60:.1f} hours\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a64f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create user tag profile (weighted by playtime)\n",
    "# Also build disliked profile from games with low playtime\n",
    "user_tag_profile = {}\n",
    "total_playtime = loved_games['playtime_forever'].sum()\n",
    "\n",
    "# NSFW and meta tag filters (same as model_development)\n",
    "nsfw_tags = {\n",
    "    'Sexual Content', 'Nudity', 'NSFW', 'Adult',\n",
    "    'Hentai', 'Erotic', 'Sexual', 'Porn', '18+', 'Adult Only'\n",
    "}\n",
    "\n",
    "meta_tags = {\n",
    "    'Indie', 'Casual', 'Free to Play', 'Early Access',\n",
    "    'Great Soundtrack', 'Singleplayer', 'Multiplayer',\n",
    "    'Co-op', 'Online Co-Op', 'PvP', 'PvE',\n",
    "    'Moddable', 'Controller', 'Partial Controller Support',\n",
    "    'Steam Achievements', 'Steam Cloud', 'Steam Trading Cards',\n",
    "    'VR', 'VR Only',\n",
    "    'Anime', 'Cute', 'Funny', 'Comedy',\n",
    "    'Classic', 'Remake', 'Remaster', 'Retro'\n",
    "}\n",
    "\n",
    "# Build loved tag profile\n",
    "for _, game in loved_games.iterrows():\n",
    "    playtime_weight = game['playtime_forever'] / total_playtime\n",
    "    for tag, votes in game['tags_dict'].items():\n",
    "        # Skip NSFW and meta tags\n",
    "        if tag in nsfw_tags or tag in meta_tags:\n",
    "            continue\n",
    "        if tag not in user_tag_profile:\n",
    "            user_tag_profile[tag] = 0\n",
    "        user_tag_profile[tag] += votes * playtime_weight\n",
    "\n",
    "# Build disliked tag profile (tags that appear in low-playtime games)\n",
    "disliked_tag_profile = {}\n",
    "for _, game in disliked_games.iterrows():\n",
    "    for tag in game['tags_dict'].keys():\n",
    "        # Skip NSFW and meta tags\n",
    "        if tag in nsfw_tags or tag in meta_tags:\n",
    "            continue\n",
    "        disliked_tag_profile[tag] = disliked_tag_profile.get(tag, 0) + 1\n",
    "\n",
    "# Remove overlaps with loved tags (don't penalize tags you also love)\n",
    "loved_tag_set = set(user_tag_profile.keys())\n",
    "disliked_tag_profile = {tag: count for tag, count in disliked_tag_profile.items() \n",
    "                       if tag not in loved_tag_set and count >= 3}  # At least 3 games to be confident\n",
    "\n",
    "# Sort by importance\n",
    "top_user_tags = sorted(user_tag_profile.items(), key=lambda x: x[1], reverse=True)[:30]\n",
    "\n",
    "print(\"\\n\\nYour top 30 tags (weighted by playtime, excluding NSFW/meta):\")\n",
    "for tag, score in top_user_tags:\n",
    "    print(f\"  {tag}: {score:.1f}\")\n",
    "\n",
    "if disliked_tag_profile:\n",
    "    print(f\"\\n\\nDisliked tags (appear in unplayed games, NOT in loved games):\")\n",
    "    for tag, count in sorted(disliked_tag_profile.items(), key=lambda x: x[1], reverse=True)[:10]:\n",
    "        print(f\"  - {tag}: {count} games (will be penalized)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002523c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create user genre profile\n",
    "# Also build disliked genre profile\n",
    "user_genre_profile = {}\n",
    "disliked_genre_profile = {}\n",
    "\n",
    "# Meta genres to exclude from matching\n",
    "meta_genres = {\n",
    "    'Indie', 'Casual', 'Early Access', 'Free to Play',\n",
    "    'Massively Multiplayer',\n",
    "    'Utilities', 'Software', 'Animation & Modeling', 'Design & Illustration',\n",
    "    'Audio Production', 'Video Production', 'Web Publishing', 'Education',\n",
    "    'Photo Editing', 'Game Development'\n",
    "}\n",
    "\n",
    "for _, game in loved_games.iterrows():\n",
    "    playtime_weight = game['playtime_forever'] / total_playtime\n",
    "    for genre in game['genre_list']:\n",
    "        # Skip meta genres\n",
    "        if genre in meta_genres:\n",
    "            continue\n",
    "        if genre not in user_genre_profile:\n",
    "            user_genre_profile[genre] = 0\n",
    "        user_genre_profile[genre] += playtime_weight\n",
    "\n",
    "# Build disliked genre profile\n",
    "for _, game in disliked_games.iterrows():\n",
    "    for genre in game['genre_list']:\n",
    "        # Skip meta genres\n",
    "        if genre in meta_genres:\n",
    "            continue\n",
    "        disliked_genre_profile[genre] = disliked_genre_profile.get(genre, 0) + 1\n",
    "\n",
    "# Remove overlaps\n",
    "loved_genre_set = set(user_genre_profile.keys())\n",
    "disliked_genre_profile = {genre: count for genre, count in disliked_genre_profile.items() \n",
    "                         if genre not in loved_genre_set and count >= 3}\n",
    "\n",
    "print(\"\\n\\nYour genre preferences (excluding meta genres):\")\n",
    "for genre, score in sorted(user_genre_profile.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"  {genre}: {score:.2%}\")\n",
    "\n",
    "if disliked_genre_profile:\n",
    "    print(f\"\\n\\nDisliked genres (will be penalized):\")\n",
    "    for genre, count in sorted(disliked_genre_profile.items(), key=lambda x: x[1], reverse=True):\n",
    "        print(f\"  - {genre}: {count} games\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35c6d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate content-based scores for catalog games (ADVANCED VERSION)\n",
    "def calculate_content_score(game_row, user_tag_profile, user_genre_profile, loved_games, \n",
    "                            disliked_tag_profile, disliked_genre_profile):\n",
    "    \"\"\"\n",
    "    Calculate content-based score (0-100) for a single game\n",
    "    \n",
    "    Components:\n",
    "    - Tag similarity: 45 points (MOST IMPORTANT - specific gameplay features)\n",
    "    - Genre overlap: 20 points (broad categories)\n",
    "    - Median playtime match: 20 points (engagement signal)\n",
    "    - Review quality: 15 points (community sentiment)\n",
    "    \n",
    "    Penalties:\n",
    "    - Disliked tags: -10 points per matching disliked tag (soft penalty)\n",
    "    - Disliked genres: -15 points per matching disliked genre (soft penalty)\n",
    "    - NSFW tags: Auto-exclude (hard filter, not scored)\n",
    "    - Early Access: Can be filtered out (hard filter, not scored)\n",
    "    \"\"\"\n",
    "    score = 0\n",
    "    \n",
    "    # NSFW filter - these tags auto-disqualify\n",
    "    nsfw_tags = {\n",
    "        'Sexual Content', 'Nudity', 'NSFW', 'Adult',\n",
    "        'Hentai', 'Erotic', 'Sexual', 'Porn', '18+', 'Adult Only'\n",
    "    }\n",
    "    \n",
    "    meta_tags = {\n",
    "        'Indie', 'Casual', 'Free to Play', 'Early Access',\n",
    "        'Great Soundtrack', 'Singleplayer', 'Multiplayer',\n",
    "        'Co-op', 'Online Co-Op', 'PvP', 'PvE',\n",
    "        'Moddable', 'Controller', 'Partial Controller Support',\n",
    "        'Steam Achievements', 'Steam Cloud', 'Steam Trading Cards',\n",
    "        'VR', 'VR Only',\n",
    "        'Anime', 'Cute', 'Funny', 'Comedy',\n",
    "        'Classic', 'Remake', 'Remaster', 'Retro'\n",
    "    }\n",
    "    \n",
    "    meta_genres = {\n",
    "        'Indie', 'Casual', 'Early Access', 'Free to Play',\n",
    "        'Massively Multiplayer',\n",
    "        'Utilities', 'Software', 'Animation & Modeling', 'Design & Illustration',\n",
    "        'Audio Production', 'Video Production', 'Web Publishing', 'Education',\n",
    "        'Photo Editing', 'Game Development'\n",
    "    }\n",
    "    \n",
    "    # Check for NSFW content (hard filter - return 0 if found)\n",
    "    game_tags = game_row['tags_dict']\n",
    "    if any(tag in nsfw_tags for tag in game_tags.keys()):\n",
    "        return 0  # NSFW content auto-disqualified\n",
    "    \n",
    "    # 1. Tag similarity (45 points) - MOST IMPORTANT\n",
    "    tag_score = 0\n",
    "    tag_penalty = 0\n",
    "    if game_tags:\n",
    "        for tag, votes in game_tags.items():\n",
    "            # Skip NSFW and meta tags from scoring\n",
    "            if tag in nsfw_tags or tag in meta_tags:\n",
    "                continue\n",
    "            \n",
    "            # Positive matches\n",
    "            if tag in user_tag_profile:\n",
    "                tag_score += user_tag_profile[tag] * (votes / 1000)  # Normalize votes\n",
    "            \n",
    "            # Negative matches (soft penalty)\n",
    "            if tag in disliked_tag_profile:\n",
    "                tag_penalty += 10  # -10 points per disliked tag\n",
    "        \n",
    "        # Normalize tag score to 0-45 range\n",
    "        max_possible_tag_score = sum(user_tag_profile.values()) * 10  # Approximate max\n",
    "        tag_score = min(45, (tag_score / max_possible_tag_score) * 45) if max_possible_tag_score > 0 else 0\n",
    "    \n",
    "    score += tag_score\n",
    "    score -= tag_penalty  # Apply soft penalty\n",
    "    \n",
    "    # 2. Genre overlap (20 points)\n",
    "    genre_score = 0\n",
    "    genre_penalty = 0\n",
    "    game_genres = game_row['genre_list']\n",
    "    if game_genres:\n",
    "        for genre in game_genres:\n",
    "            # Skip meta genres\n",
    "            if genre in meta_genres:\n",
    "                continue\n",
    "            \n",
    "            # Positive matches\n",
    "            if genre in user_genre_profile:\n",
    "                genre_score += user_genre_profile[genre]\n",
    "            \n",
    "            # Negative matches (soft penalty)\n",
    "            if genre in disliked_genre_profile:\n",
    "                genre_penalty += 15  # -15 points per disliked genre\n",
    "        \n",
    "        genre_score = min(20, genre_score * 20)  # Normalize to 0-20\n",
    "    \n",
    "    score += genre_score\n",
    "    score -= genre_penalty  # Apply soft penalty\n",
    "    \n",
    "    # 3. Median playtime similarity (20 points)\n",
    "    # Higher median = deeper, more engaging game\n",
    "    median_playtime = game_row.get('median_forever', 0)\n",
    "    if pd.notna(median_playtime) and median_playtime > 0:\n",
    "        median_hours = median_playtime / 60\n",
    "        # Games with high median playtime get higher scores\n",
    "        if median_hours >= 50:\n",
    "            playtime_score = 20  # Deep, engaging game\n",
    "        elif median_hours >= 20:\n",
    "            playtime_score = 15  # Good engagement\n",
    "        elif median_hours >= 10:\n",
    "            playtime_score = 10  # Decent engagement\n",
    "        elif median_hours >= 5:\n",
    "            playtime_score = 5   # Some engagement\n",
    "        else:\n",
    "            playtime_score = 0   # Low engagement\n",
    "    else:\n",
    "        playtime_score = 0\n",
    "    score += playtime_score\n",
    "    \n",
    "    # 4. Review quality (15 points)\n",
    "    positive = game_row.get('positive', 0)\n",
    "    negative = game_row.get('negative', 0)\n",
    "    total_reviews = positive + negative\n",
    "    \n",
    "    if total_reviews > 0:\n",
    "        positive_ratio = positive / total_reviews\n",
    "        review_percentage = positive_ratio * 100\n",
    "        \n",
    "        # Quality tiers (matching model_development logic)\n",
    "        if review_percentage >= 95:\n",
    "            quality_multiplier = 2.5  # Overwhelmingly Positive\n",
    "        elif review_percentage >= 90:\n",
    "            quality_multiplier = 2.0  # Very Positive\n",
    "        elif review_percentage >= 80:\n",
    "            quality_multiplier = 1.5  # Mostly Positive\n",
    "        elif review_percentage >= 70:\n",
    "            quality_multiplier = 1.0  # Positive\n",
    "        elif review_percentage >= 60:\n",
    "            quality_multiplier = 0.5  # Mixed (penalize)\n",
    "        else:\n",
    "            quality_multiplier = 0.1  # Negative/Mostly Negative (heavy penalty)\n",
    "        \n",
    "        # Volume bonus (logarithmic - more reviews = more reliable)\n",
    "        volume_score = np.log10(total_reviews + 1) * quality_multiplier\n",
    "        review_score = min(15, volume_score * 1.5)  # Scale to 0-15 range\n",
    "    else:\n",
    "        review_score = 0\n",
    "    score += review_score\n",
    "    \n",
    "    # Ensure score doesn't go below 0\n",
    "    return max(0, score)\n",
    "\n",
    "print(\"Calculating content-based scores for catalog games...\")\n",
    "print(\"This may take a minute...\")\n",
    "print(\"\\nFeatures:\")\n",
    "print(\"  âœ… Tag matching (45%) - specific gameplay features\")\n",
    "print(\"  âœ… Genre matching (20%) - broad categories\")\n",
    "print(\"  âœ… Median playtime (20%) - engagement signal\")\n",
    "print(\"  âœ… Review quality (15%) - community sentiment\")\n",
    "print(\"  âœ… NSFW filter (hard exclusion)\")\n",
    "print(\"  âœ… Meta tag/genre filtering (excluded from matching)\")\n",
    "print(\"  âœ… Disliked tag penalties (soft -10 points per tag)\")\n",
    "print(\"  âœ… Disliked genre penalties (soft -15 points per genre)\")\n",
    "\n",
    "catalog_unowned['content_score'] = catalog_unowned.apply(\n",
    "    lambda row: calculate_content_score(\n",
    "        row, user_tag_profile, user_genre_profile, loved_games,\n",
    "        disliked_tag_profile, disliked_genre_profile\n",
    "    ),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ“ Content-based scores calculated!\")\n",
    "print(f\"\\nContent Score Statistics:\")\n",
    "print(catalog_unowned['content_score'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4805eeb6",
   "metadata": {},
   "source": [
    "## 5. Merge ML and Content-Based Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdac6d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge ML scores with catalog\n",
    "catalog_with_scores = catalog_unowned.merge(\n",
    "    ml_scores,\n",
    "    on='appid',\n",
    "    how='left',\n",
    "    suffixes=('', '_ml')\n",
    ")\n",
    "\n",
    "# Fill missing ML scores with 0 (games not in ML recommendations)\n",
    "catalog_with_scores['ml_score'] = catalog_with_scores['ml_score'].fillna(0)\n",
    "\n",
    "print(f\"Combined dataset: {len(catalog_with_scores)} games\")\n",
    "print(f\"Games with ML scores: {(catalog_with_scores['ml_score'] > 0).sum()}\")\n",
    "print(f\"Games with content scores: {(catalog_with_scores['content_score'] > 0).sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7623b797",
   "metadata": {},
   "source": [
    "## 6. Calculate Hybrid Scores\n",
    "\n",
    "Combine ML and content-based scores with configurable weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9afc243b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hybrid scoring weights (can be tuned)\n",
    "WEIGHT_ML = 0.40\n",
    "WEIGHT_CONTENT = 0.40\n",
    "WEIGHT_REVIEW = 0.20\n",
    "\n",
    "print(f\"Hybrid Weights:\")\n",
    "print(f\"  ML: {WEIGHT_ML:.0%}\")\n",
    "print(f\"  Content-Based: {WEIGHT_CONTENT:.0%}\")\n",
    "print(f\"  Review Quality: {WEIGHT_REVIEW:.0%}\")\n",
    "print(f\"  Total: {WEIGHT_ML + WEIGHT_CONTENT + WEIGHT_REVIEW:.0%}\")\n",
    "\n",
    "# Calculate review quality score (0-100 scale)\n",
    "def calculate_review_score(row):\n",
    "    positive = row.get('positive', 0)\n",
    "    negative = row.get('negative', 0)\n",
    "    total = positive + negative\n",
    "    \n",
    "    if total == 0:\n",
    "        return 0\n",
    "    \n",
    "    positive_ratio = positive / total\n",
    "    # Base score: positive ratio (0-80 points)\n",
    "    score = positive_ratio * 80\n",
    "    # Volume bonus (0-20 points)\n",
    "    volume_bonus = min(20, (np.log1p(total) / np.log1p(100000)) * 20)\n",
    "    \n",
    "    return score + volume_bonus\n",
    "\n",
    "catalog_with_scores['review_score'] = catalog_with_scores.apply(calculate_review_score, axis=1)\n",
    "\n",
    "# Calculate hybrid score\n",
    "catalog_with_scores['hybrid_score'] = (\n",
    "    WEIGHT_ML * catalog_with_scores['ml_score'] +\n",
    "    WEIGHT_CONTENT * catalog_with_scores['content_score'] +\n",
    "    WEIGHT_REVIEW * catalog_with_scores['review_score']\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ“ Hybrid scores calculated!\")\n",
    "print(f\"\\nScore Statistics:\")\n",
    "print(catalog_with_scores[['ml_score', 'content_score', 'review_score', 'hybrid_score']].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d612e44f",
   "metadata": {},
   "source": [
    "## 6.5 Optional: Apply User Preference Boosts/Hard Exclusions\n",
    "\n",
    "You can further customize recommendations by:\n",
    "- **Boosting** specific genres/tags you want to see more of (+5 to +20 points)\n",
    "- **Hard excluding** genres/tags you absolutely don't want (removes from candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc71f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Apply user preference adjustments\n",
    "# Uncomment and customize these to your preferences\n",
    "\n",
    "# Example: Boost specific genres/tags you want to see more of\n",
    "BOOST_GENRES = {}  # e.g., {'RPG': 10, 'Action': 5}\n",
    "BOOST_TAGS = {}    # e.g., {'Open World': 15, 'Multiplayer': 10}\n",
    "\n",
    "# Example: Hard exclude genres/tags you never want to see\n",
    "HARD_EXCLUDE_GENRES = []  # e.g., ['Sports', 'Racing']\n",
    "HARD_EXCLUDE_TAGS = []    # e.g., ['Horror', 'Survival Horror', '2D']\n",
    "\n",
    "def apply_preference_adjustments(df, boost_genres, boost_tags, exclude_genres, exclude_tags):\n",
    "    \"\"\"\n",
    "    Apply preference boosts and hard exclusions\n",
    "    \n",
    "    Returns:\n",
    "        - df with adjusted scores\n",
    "        - df with hard exclusions removed\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Apply boosts\n",
    "    if boost_genres or boost_tags:\n",
    "        print(f\"\\nApplying preference boosts...\")\n",
    "        \n",
    "        for idx, row in df.iterrows():\n",
    "            boost = 0\n",
    "            \n",
    "            # Genre boosts\n",
    "            for genre in row['genre_list']:\n",
    "                if genre in boost_genres:\n",
    "                    boost += boost_genres[genre]\n",
    "            \n",
    "            # Tag boosts\n",
    "            for tag in row['tags_dict'].keys():\n",
    "                if tag in boost_tags:\n",
    "                    boost += boost_tags[tag]\n",
    "            \n",
    "            # Apply boost to all scores\n",
    "            if boost > 0:\n",
    "                df.at[idx, 'ml_score'] += boost * 0.4\n",
    "                df.at[idx, 'content_score'] += boost * 0.4\n",
    "                df.at[idx, 'hybrid_score'] += boost\n",
    "    \n",
    "    # Apply hard exclusions\n",
    "    if exclude_genres or exclude_tags:\n",
    "        print(f\"\\nApplying hard exclusions...\")\n",
    "        before_exclude = len(df)\n",
    "        \n",
    "        if exclude_genres:\n",
    "            df = df[~df['genre_list'].apply(lambda x: any(g in exclude_genres for g in x))]\n",
    "            print(f\"  - Excluded {before_exclude - len(df)} games with genres: {exclude_genres}\")\n",
    "        \n",
    "        if exclude_tags:\n",
    "            before_tag_exclude = len(df)\n",
    "            df = df[~df['tags_dict'].apply(lambda x: any(t in exclude_tags for t in x.keys()))]\n",
    "            print(f\"  - Excluded {before_tag_exclude - len(df)} games with tags: {exclude_tags}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply adjustments to catalog\n",
    "catalog_with_scores = apply_preference_adjustments(\n",
    "    catalog_with_scores,\n",
    "    BOOST_GENRES,\n",
    "    BOOST_TAGS,\n",
    "    HARD_EXCLUDE_GENRES,\n",
    "    HARD_EXCLUDE_TAGS\n",
    ")\n",
    "\n",
    "# Recalculate hybrid scores if boosts were applied\n",
    "if BOOST_GENRES or BOOST_TAGS:\n",
    "    catalog_with_scores['hybrid_score'] = (\n",
    "        WEIGHT_ML * catalog_with_scores['ml_score'] +\n",
    "        WEIGHT_CONTENT * catalog_with_scores['content_score'] +\n",
    "        WEIGHT_REVIEW * catalog_with_scores['review_score']\n",
    "    )\n",
    "    print(f\"\\nâœ“ Hybrid scores recalculated with preference boosts!\")\n",
    "\n",
    "print(f\"\\nâœ“ Preference adjustments applied!\")\n",
    "print(f\"Remaining candidates: {len(catalog_with_scores)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eadff939",
   "metadata": {},
   "source": [
    "## 7. Generate Top 20 Recommendations (All 3 Approaches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f868107",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply universal filters (quality gates) - matching model_development logic\n",
    "MIN_REVIEWS = 1000\n",
    "MIN_REVIEW_SCORE = 70  # % positive\n",
    "EXCLUDE_EARLY_ACCESS = True\n",
    "SFW_ONLY = True\n",
    "\n",
    "print(f\"Applying quality filters:\")\n",
    "print(f\"  - Minimum reviews: {MIN_REVIEWS}\")\n",
    "print(f\"  - Minimum review score: {MIN_REVIEW_SCORE}%\")\n",
    "print(f\"  - Exclude Early Access: {EXCLUDE_EARLY_ACCESS}\")\n",
    "print(f\"  - SFW only: {SFW_ONLY}\")\n",
    "\n",
    "filtered_catalog = catalog_with_scores[\n",
    "    ((catalog_with_scores['positive'] + catalog_with_scores['negative']) >= MIN_REVIEWS) &\n",
    "    ((catalog_with_scores['positive'] / (catalog_with_scores['positive'] + catalog_with_scores['negative']) * 100) >= MIN_REVIEW_SCORE) &\n",
    "    (catalog_with_scores['content_score'] > 0)  # Exclude games with 0 content score (NSFW filtered)\n",
    "].copy()\n",
    "\n",
    "# Apply Early Access filter\n",
    "if EXCLUDE_EARLY_ACCESS:\n",
    "    before_ea = len(filtered_catalog)\n",
    "    filtered_catalog = filtered_catalog[\n",
    "        ~filtered_catalog['genre_list'].apply(lambda x: 'Early Access' in x)\n",
    "    ]\n",
    "    print(f\"  - Filtered out {before_ea - len(filtered_catalog)} Early Access games\")\n",
    "\n",
    "print(f\"\\nAfter quality filters: {len(filtered_catalog)} games\")\n",
    "print(f\"  (min {MIN_REVIEWS} reviews, min {MIN_REVIEW_SCORE}% positive)\")\n",
    "\n",
    "# Get top 20 for each approach\n",
    "top_20_ml = filtered_catalog.nlargest(20, 'ml_score')\n",
    "top_20_content = filtered_catalog.nlargest(20, 'content_score')\n",
    "top_20_hybrid = filtered_catalog.nlargest(20, 'hybrid_score')\n",
    "\n",
    "print(f\"\\nâœ“ Top 20 recommendations generated for all 3 approaches!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a84cdd8",
   "metadata": {},
   "source": [
    "## 8. Compare Approaches Side-by-Side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3e608c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_recommendations(df, approach_name, score_col):\n",
    "    \"\"\"\n",
    "    Display top 20 recommendations in a readable format\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(f\"TOP 20 RECOMMENDATIONS - {approach_name.upper()}\")\n",
    "    print(\"=\"*100)\n",
    "    \n",
    "    for idx, (_, row) in enumerate(df.iterrows(), 1):\n",
    "        # Calculate review score\n",
    "        total_reviews = row['positive'] + row['negative']\n",
    "        review_pct = (row['positive'] / total_reviews * 100) if total_reviews > 0 else 0\n",
    "        \n",
    "        # Get top 3 tags\n",
    "        game_tags = row['tags_dict']\n",
    "        if game_tags:\n",
    "            top_tags = sorted(game_tags.items(), key=lambda x: x[1], reverse=True)[:3]\n",
    "            tag_names = [t[0] for t in top_tags]\n",
    "        else:\n",
    "            tag_names = []\n",
    "        \n",
    "        print(f\"\\n{idx}. {row['name']}\")\n",
    "        print(f\"   Score: {row[score_col]:.1f}/100\")\n",
    "        if approach_name == 'Hybrid':\n",
    "            print(f\"   (ML: {row['ml_score']:.1f} | Content: {row['content_score']:.1f} | Review: {row['review_score']:.1f})\")\n",
    "        print(f\"   Reviews: {review_pct:.1f}% positive ({int(total_reviews):,} total)\")\n",
    "        print(f\"   Genre: {row['genre']}\")\n",
    "        print(f\"   Tags: {', '.join(tag_names)}\")\n",
    "        print(f\"   Median Playtime: {row.get('median_forever', 0):.0f} min\")\n",
    "        print(f\"   Price: ${row.get('price', 0)/100:.2f}\")\n",
    "\n",
    "# Display all three approaches\n",
    "display_recommendations(top_20_ml, 'Pure ML', 'ml_score')\n",
    "display_recommendations(top_20_content, 'Pure Content-Based', 'content_score')\n",
    "display_recommendations(top_20_hybrid, 'Hybrid (40% ML + 40% Content + 20% Review)', 'hybrid_score')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da46acc9",
   "metadata": {},
   "source": [
    "## 9. Analyze Overlap Between Approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249a4ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get app IDs for each approach\n",
    "ml_appids = set(top_20_ml['appid'].values)\n",
    "content_appids = set(top_20_content['appid'].values)\n",
    "hybrid_appids = set(top_20_hybrid['appid'].values)\n",
    "\n",
    "# Calculate overlaps\n",
    "ml_content_overlap = ml_appids.intersection(content_appids)\n",
    "ml_hybrid_overlap = ml_appids.intersection(hybrid_appids)\n",
    "content_hybrid_overlap = content_appids.intersection(hybrid_appids)\n",
    "all_three_overlap = ml_appids.intersection(content_appids).intersection(hybrid_appids)\n",
    "\n",
    "print(\"OVERLAP ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nML vs Content-Based: {len(ml_content_overlap)}/20 games in common ({len(ml_content_overlap)/20*100:.0f}%)\")\n",
    "print(f\"ML vs Hybrid: {len(ml_hybrid_overlap)}/20 games in common ({len(ml_hybrid_overlap)/20*100:.0f}%)\")\n",
    "print(f\"Content vs Hybrid: {len(content_hybrid_overlap)}/20 games in common ({len(content_hybrid_overlap)/20*100:.0f}%)\")\n",
    "print(f\"\\nAll 3 approaches agree on: {len(all_three_overlap)} games ({len(all_three_overlap)/20*100:.0f}%)\")\n",
    "\n",
    "if all_three_overlap:\n",
    "    print(f\"\\nGames that appear in ALL 3 top 20 lists:\")\n",
    "    consensus_games = catalog_with_scores[catalog_with_scores['appid'].isin(all_three_overlap)]\n",
    "    for _, game in consensus_games.iterrows():\n",
    "        print(f\"  - {game['name']}\")\n",
    "\n",
    "# Unique recommendations per approach\n",
    "ml_unique = ml_appids - content_appids - hybrid_appids\n",
    "content_unique = content_appids - ml_appids - hybrid_appids\n",
    "hybrid_unique = hybrid_appids - ml_appids - content_appids\n",
    "\n",
    "print(f\"\\nUnique to ML only: {len(ml_unique)} games\")\n",
    "print(f\"Unique to Content only: {len(content_unique)} games\")\n",
    "print(f\"Unique to Hybrid only: {len(hybrid_unique)} games\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76371c06",
   "metadata": {},
   "source": [
    "## 10. Visualize Score Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9bed6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# ML scores\n",
    "axes[0, 0].hist(filtered_catalog['ml_score'], bins=30, edgecolor='black', alpha=0.7)\n",
    "axes[0, 0].axvline(top_20_ml['ml_score'].min(), color='red', linestyle='--', label='Top 20 cutoff')\n",
    "axes[0, 0].set_xlabel('ML Score')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "axes[0, 0].set_title('ML Score Distribution')\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# Content scores\n",
    "axes[0, 1].hist(filtered_catalog['content_score'], bins=30, edgecolor='black', alpha=0.7, color='green')\n",
    "axes[0, 1].axvline(top_20_content['content_score'].min(), color='red', linestyle='--', label='Top 20 cutoff')\n",
    "axes[0, 1].set_xlabel('Content Score')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "axes[0, 1].set_title('Content-Based Score Distribution')\n",
    "axes[0, 1].legend()\n",
    "\n",
    "# Hybrid scores\n",
    "axes[1, 0].hist(filtered_catalog['hybrid_score'], bins=30, edgecolor='black', alpha=0.7, color='purple')\n",
    "axes[1, 0].axvline(top_20_hybrid['hybrid_score'].min(), color='red', linestyle='--', label='Top 20 cutoff')\n",
    "axes[1, 0].set_xlabel('Hybrid Score')\n",
    "axes[1, 0].set_ylabel('Frequency')\n",
    "axes[1, 0].set_title('Hybrid Score Distribution')\n",
    "axes[1, 0].legend()\n",
    "\n",
    "# Score correlation scatter\n",
    "axes[1, 1].scatter(filtered_catalog['ml_score'], filtered_catalog['content_score'], alpha=0.3)\n",
    "axes[1, 1].set_xlabel('ML Score')\n",
    "axes[1, 1].set_ylabel('Content Score')\n",
    "axes[1, 1].set_title('ML vs Content Score Correlation')\n",
    "correlation = filtered_catalog[['ml_score', 'content_score']].corr().iloc[0, 1]\n",
    "axes[1, 1].text(0.05, 0.95, f'Correlation: {correlation:.3f}', \n",
    "               transform=axes[1, 1].transAxes, verticalalignment='top')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nML vs Content-Based correlation: {correlation:.3f}\")\n",
    "if correlation < 0.3:\n",
    "    print(\"â†’ Low correlation: ML and Content capture DIFFERENT signals (good for hybrid!)\")\n",
    "elif correlation > 0.7:\n",
    "    print(\"â†’ High correlation: ML and Content agree strongly (hybrid may not add much)\")\n",
    "else:\n",
    "    print(\"â†’ Moderate correlation: ML and Content complement each other (hybrid promising!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf56370",
   "metadata": {},
   "source": [
    "## 11. Save Hybrid Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94cfacde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save top 20 for each approach\n",
    "top_20_ml[['appid', 'name', 'ml_score', 'genre', 'positive', 'negative', 'median_forever', 'price']].to_csv(\n",
    "    '../data/recommendations_ml_only.csv', index=False\n",
    ")\n",
    "\n",
    "top_20_content[['appid', 'name', 'content_score', 'genre', 'positive', 'negative', 'median_forever', 'price']].to_csv(\n",
    "    '../data/recommendations_content_only.csv', index=False\n",
    ")\n",
    "\n",
    "top_20_hybrid[['appid', 'name', 'hybrid_score', 'ml_score', 'content_score', 'review_score', \n",
    "               'genre', 'positive', 'negative', 'median_forever', 'price']].to_csv(\n",
    "    '../data/recommendations_hybrid.csv', index=False\n",
    ")\n",
    "\n",
    "print(\"âœ“ All recommendations saved!\")\n",
    "print(\"\\nFiles created:\")\n",
    "print(\"  - recommendations_ml_only.csv\")\n",
    "print(\"  - recommendations_content_only.csv\")\n",
    "print(\"  - recommendations_hybrid.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7cc65a",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### âœ… Hybrid System Complete with Advanced Filtering!\n",
    "\n",
    "**What We Built**:\n",
    "1. Pure ML recommendations (learned from your playtime patterns)\n",
    "2. Pure content-based recommendations (similarity to your loved games)\n",
    "3. Hybrid recommendations (weighted combination)\n",
    "\n",
    "**Advanced Features Included** (matching model_development.ipynb):\n",
    "\n",
    "**ðŸ”’ Hard Filters** (auto-exclude before scoring):\n",
    "- âœ… NSFW content filter (Sexual Content, Nudity, etc.)\n",
    "- âœ… Early Access filter (exclude unfinished games)\n",
    "- âœ… Meta genre filter (Utilities, Software, etc. excluded from matching)\n",
    "- âœ… Meta tag filter (Indie, Casual, etc. excluded from matching)\n",
    "- âœ… Minimum review count (1000+ reviews)\n",
    "- âœ… Minimum review score (70%+ positive)\n",
    "- âœ… Hard exclusions (user-specified genres/tags never shown)\n",
    "\n",
    "**ðŸŽ¯ Soft Scoring** (influence recommendations):\n",
    "- âœ… Tag matching (45%) - weighted by YOUR playtime on each tag\n",
    "- âœ… Genre matching (20%) - broad category preferences\n",
    "- âœ… Median playtime (20%) - community engagement signal\n",
    "- âœ… Review quality (15%) - community sentiment with quality tiers\n",
    "- âœ… Disliked tag penalties (-10 points per tag from unplayed games)\n",
    "- âœ… Disliked genre penalties (-15 points per genre from unplayed games)\n",
    "- âœ… User preference boosts (+5 to +20 points for preferred genres/tags)\n",
    "\n",
    "**Key Findings**:\n",
    "- ML vs Content correlation: [see output above]\n",
    "- Overlap between approaches: [see output above]\n",
    "- Best weight configuration: **To be determined based on your evaluation**\n",
    "\n",
    "**Filtering Logic**:\n",
    "```\n",
    "1. Apply universal filters (NSFW, Early Access, min reviews, meta genres)\n",
    "2. Calculate ML score (learned from playtime)\n",
    "3. Calculate Content score (similarity + penalties for disliked features)\n",
    "4. Calculate Review score (quality + volume)\n",
    "5. Apply user preference boosts (optional)\n",
    "6. Combine into hybrid score\n",
    "7. Apply hard exclusions (user-specified genres/tags to never show)\n",
    "8. Return top 20\n",
    "```\n",
    "\n",
    "**Next Steps**:\n",
    "1. âœ… Evaluate recommendations manually (which approach is best?)\n",
    "2. Adjust hybrid weights based on your preferences\n",
    "3. Set hard exclusions for genres/tags you never want to see\n",
    "4. Set preference boosts for genres/tags you want to see more of\n",
    "5. Add diversity filter to reduce similar games in top 20 (from feature_engineering.ipynb)\n",
    "6. Build web interface for easier interaction\n",
    "\n",
    "**Files Created**:\n",
    "- `recommendations_ml_only.csv` - Pure ML top 20\n",
    "- `recommendations_content_only.csv` - Pure content-based top 20\n",
    "- `recommendations_hybrid.csv` - Hybrid top 20 with all advanced features\n",
    "\n",
    "**How This Differs from Basic Content-Based**:\n",
    "- âœ… Learns dislike patterns from low-playtime games (soft penalties)\n",
    "- âœ… Filters out NSFW, meta genres, Early Access automatically\n",
    "- âœ… Combines ML predictions with content similarity\n",
    "- âœ… Supports hard exclusions for genres/tags you hate\n",
    "- âœ… Supports preference boosts for genres/tags you want more of\n",
    "- âœ… More accurate tag/genre matching (excludes meta tags like \"Indie\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec80ea8",
   "metadata": {},
   "source": [
    "## 13. Experiment: Tune Hybrid Weights\n",
    "\n",
    "Try different weight combinations to see which works best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7cd5afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define weight combinations to try\n",
    "weight_experiments = [\n",
    "    {'name': 'ML Heavy', 'ml': 0.50, 'content': 0.30, 'review': 0.20},\n",
    "    {'name': 'Content Heavy', 'ml': 0.30, 'content': 0.50, 'review': 0.20},\n",
    "    {'name': 'Balanced', 'ml': 0.40, 'content': 0.40, 'review': 0.20},\n",
    "    {'name': 'Review Heavy', 'ml': 0.35, 'content': 0.35, 'review': 0.30},\n",
    "]\n",
    "\n",
    "print(\"WEIGHT TUNING EXPERIMENTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for exp in weight_experiments:\n",
    "    # Calculate hybrid score with these weights\n",
    "    catalog_with_scores[f'hybrid_{exp[\"name\"].lower().replace(\" \", \"_\")}'] = (\n",
    "        exp['ml'] * catalog_with_scores['ml_score'] +\n",
    "        exp['content'] * catalog_with_scores['content_score'] +\n",
    "        exp['review'] * catalog_with_scores['review_score']\n",
    "    )\n",
    "    \n",
    "    # Get top 10 for this configuration\n",
    "    col_name = f'hybrid_{exp[\"name\"].lower().replace(\" \", \"_\")}'\n",
    "    top_10 = filtered_catalog.nlargest(10, col_name)\n",
    "    \n",
    "    print(f\"\\n{exp['name']} (ML: {exp['ml']:.0%}, Content: {exp['content']:.0%}, Review: {exp['review']:.0%})\")\n",
    "    print(\"-\" * 80)\n",
    "    for idx, (_, row) in enumerate(top_10.iterrows(), 1):\n",
    "        print(f\"{idx}. {row['name']} (Score: {row[col_name]:.1f})\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Compare the lists above and see which weight combination appeals most to you!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9162d568",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### âœ… Hybrid System Complete!\n",
    "\n",
    "**What We Built**:\n",
    "1. Pure ML recommendations (learned from your playtime patterns)\n",
    "2. Pure content-based recommendations (similarity to your loved games)\n",
    "3. Hybrid recommendations (weighted combination)\n",
    "\n",
    "**Key Findings**:\n",
    "- ML vs Content correlation: [see output above]\n",
    "- Overlap between approaches: [see output above]\n",
    "- Best weight configuration: **To be determined based on your evaluation**\n",
    "\n",
    "**Next Steps**:\n",
    "1. âœ… Evaluate recommendations manually (which approach is best?)\n",
    "2. Adjust hybrid weights based on your preferences\n",
    "3. Add diversity filter to reduce similar games in top 20\n",
    "4. Implement user preference system (boost/penalize tags/genres)\n",
    "5. Build web interface for easier interaction\n",
    "\n",
    "**Files Created**:\n",
    "- `recommendations_ml_only.csv` - Pure ML top 20\n",
    "- `recommendations_content_only.csv` - Pure content-based top 20\n",
    "- `recommendations_hybrid.csv` - Hybrid top 20"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
